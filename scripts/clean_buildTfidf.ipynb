{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data cleaning and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing done in bash:\n",
    "    * cut -d',' -f4- Sentiment\\ Analysis\\ Dataset.csv > justTweet.csv   \n",
    "    * cut -d',' -f2 Sentiment\\ Analysis\\ Dataset.csv > justSentiment.csv \n",
    "    * paste -d',' justSentiment.csv justTweet.csv > sentimentTweet.csv\n",
    "Data too large to put on git, you'll have to do the above preprocessing steps yourself to use the data for the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./sentimentTweet.csv', 'r')\n",
    "labels = []\n",
    "tweets = []\n",
    "\n",
    "for line in f:\n",
    "    labels.append(line[0])\n",
    "    tweets.append(line[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    corpus.update({i:(labels[i], tweets[i])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "#stemmer = PorterStemmer() \n",
    "\n",
    "''' I am not using the stemmer right now because the lemmatizer seems to be more appropriate for the task.\n",
    "Feel free to switch it around. '''\n",
    "\n",
    "def clean(doc):\n",
    "    #doc = \" \".join(doc)\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    #singles = \" \".join(stemmer.stem(plural) for plural in punc_free.split())\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_corpus = {}\n",
    "#doc_clean = [clean(doc).split() for doc in doc_complete]   \n",
    "for i in range(1, len(corpus)):\n",
    "    doc = (corpus[i])[1]\n",
    "    doc = clean(doc).split()\n",
    "    new_corpus.update({i-1:(labels[i], doc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('0', [u'sad', u'apl', u'friend']), [u'sad', u'apl', u'friend'])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus[0], (new_corpus[0])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "for i in range(len(new_corpus)):\n",
    "    tweet_data.append((new_corpus[i])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'sad', u'apl', u'friend']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Creating the term dictionary of the corpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(tweet_data)\n",
    "dictionary.save('/tmp/tweets.dict') \n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(tweet) for tweet in tweet_data]\n",
    "corpora.MmCorpus.serialize('/tmp/tweets.mm', doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize TF-IDF model and transform Text with TF-IDF\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create similarity matrix of all documents/tweets\n",
    "index = similarities.MatrixSimilarity(tfidf[doc_term_matrix])\n",
    "index.save('/tmp/deerwester.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims = index[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000012,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.99999994,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.99999994,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.99999994,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "For tutorial on using LDA model with Gensim:\n",
    "https://radimrehurek.com/gensim/wiki.html#latent-dirichlet-allocation \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
